{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "## Cálculo de Métricas de Cobertura con Sedona y NumPy\n",
        "\n",
        "Este notebook demuestra un flujo de trabajo completo para el análisis geoespacial distribuido. El objetivo es calcular métricas de ecología del paisaje a partir de datos de cobertura del suelo de **MapBiomas Chile**, almacenados en una base de datos PostGIS como un cubo de datos ráster.\n",
        "El proceso se estructura en un pipeline ETL (Extracción, Transformación, Carga) canónico:\n",
        "1.  **Extracción (E):** Se realiza una ingesta de datos vía JDBC desde una base de datos PostgreSQL/PostGIS. La clave de esta fase es la conversión al vuelo del tipo de dato `raster` de PostGIS a un formato `GeoTIFF` binario, lo que garantiza la compatibilidad con Apache Sedona.\n",
        "2.  **Transformación (T):**\n",
        "    - **Integración:** Se crea un Almacén de Datos Operacional (ODS) uniendo los datos ráster con datos vectoriales (comunas, grillas) para enriquecerlos y filtrarlos según una Región de Interés (ROI).\n",
        "    - **Cálculo:** Se ejecutan las métricas de paisaje (Composición, Fragmentación, Transición) utilizando **Funciones Definidas por el Usuario (UDFs) de Python**. Estas UDFs están altamente optimizadas, delegando el procesamiento numérico a librerías como **NumPy y SciPy** para un rendimiento superior.\n",
        "3.  **Carga (L):** Los resultados agregados (Data Marts) se muestran en formato de tabla, listos para su análisis o persistencia."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s1_md",
      "metadata": {},
      "source": [
        "### 1. Preparación del Entorno: Librerías y Rutas\n",
        "En esta sección, importamos las librerías necesarias y configuramos las rutas del proyecto. Esto es fundamental para asegurar que el notebook pueda localizar el driver JDBC de PostgreSQL y el archivo de configuración `.env`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s1_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerías principales\n",
        "from pathlib import Path  # Para el manejo de rutas de sistema de forma agnóstica al SO\n",
        "from sedona.spark import SedonaContext  # El punto de entrada para las funcionalidades de Sedona\n",
        "from pyspark.sql import functions as F  # Funciones built-in de Spark para manipulación de DataFrames\n",
        "from pyspark.sql.types import MapType, IntegerType, LongType, StructType, StructField, DoubleType # Tipos de datos para definir esquemas\n",
        "import numpy as np  # Librería para computación numérica, clave para el procesamiento de píxeles\n",
        "from dotenv import dotenv_values  # Para cargar variables de entorno (credenciales) desde el archivo .env\n",
        "import json # Para manejar la lista de clases en la UDF de fragmentación\n",
        "import IPython.display as display  # Para mostrar DataFrames de Spark en notebooks Jupyter\n",
        "\n",
        "# --- Configuración de Rutas ---\n",
        "# Se asume que el notebook está en la carpeta /notebook del proyecto\n",
        "notebook_dir = Path.cwd()\n",
        "project_dir = notebook_dir.parent\n",
        "env_path = project_dir / '.env' # Ruta al archivo con las credenciales de la BD\n",
        "jdbc_driver_path = str(project_dir / 'libs' / 'postgresql-42.6.0.jar') # Ruta al conector JDBC de PostgreSQL\n",
        "\n",
        "print(f'Ruta del driver JDBC: {jdbc_driver_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s2_md",
      "metadata": {},
      "source": [
        "### 2. Inicialización de Spark y Registro de UDFs\n",
        "Aquí se configura e inicializa la sesión de Spark con Sedona. La configuración de memoria es crucial para manejar grandes volúmenes de datos geoespaciales. \n",
        "A continuación, se definen y registran las UDFs que encapsulan la lógica de negocio para el cálculo de las métricas. Estas funciones convierten los datos ráster a arrays de NumPy para un procesamiento matricial eficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b4cccb",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 2.1. Configuración de la Sesión de Spark\n",
        "<small>Se configura la sesión de Spark con Sedona, ajustando la memoria para el procesamiento de datos geoespaciales y añadiendo el driver JDBC necesario. La memoria del `driver` (4g) y la memoria `Off-Heap` (2g) son cruciales para optimizar el manejo de objetos geoespaciales de gran tamaño.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e9cad4",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# --- Configuración de la Sesión de Spark ---\n",
        "# La memoria del driver (driver.memory) se ha aumentado a 4g para poder manejar operaciones \n",
        "# que colectan datos al driver, como el join espacial a nivel nacional y la visualización con .toPandas().\n",
        "# La memoria Off-Heap se habilita para que Sedona maneje geometrías y rásters fuera del Heap de la JVM, reduciendo la presión sobre el Garbage Collector.\n",
        "spark = SedonaContext.builder() \\\n",
        "    .appName(\"Sedona_Multizone_Nacional_ETL\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.jars\", f\"file://{jdbc_driver_path}\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "# Reducir verbosidad de logs para una salida más limpia\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "262a749e",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 2.2. Inicializar Sedona\n",
        "<small>Crea el contexto de Sedona, un paso crucial que habilita todas las funciones y tipos de datos geoespaciales específicos de Sedona dentro del entorno de Spark.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bf789c",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Inicializa el contexto de Sedona para registrar y utilizar sus funciones y tipos específicos\n",
        "sedona = SedonaContext.create(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42cdc321",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 2.3. UDF - Composición de Cobertura (Histograma)\n",
        "\n",
        "<small>Esta UDF calcula el histograma de valores de píxeles para una tesela de ráster. Es el pilar para la métrica de composición, ya que permite un conteo eficiente de las clases de cobertura a nivel de cada tesela.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fddb79b2",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# UDF: composición (histograma de clases)\n",
        "def get_composition_udf(raster):\n",
        "    try:\n",
        "        data = raster.as_numpy()[0]\n",
        "        unique, counts = np.unique(data, return_counts=True)\n",
        "        return {int(k): int(v) for k, v in zip(unique, counts) if not np.isnan(k)}\n",
        "    except Exception:\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d5163d",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 2.4. UDF - Fragmentación (NP, MPS)\n",
        "<small>Esta UDF utiliza `scipy.ndimage.label` para identificar componentes conectados (parches) y así calcular el Número de Parches (NP) y el Tamaño Medio del Parche (MPS) para un conjunto de clases de interés.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0678623",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# UDF: fragmentación (NP, MPS)\n",
        "def get_fragmentation_udf(raster, class_list_json):\n",
        "    try:\n",
        "        from scipy import ndimage\n",
        "        data = raster.as_numpy()[0]\n",
        "        classes = json.loads(class_list_json)\n",
        "        mask = np.isin(data, classes).astype(int)\n",
        "        _, num_features = ndimage.label(mask)\n",
        "        total_px = np.sum(mask)\n",
        "        mps = float(total_px) / num_features if num_features > 0 else 0.0\n",
        "        return {\"NP\": int(num_features), \"MPS_px\": mps}\n",
        "    except Exception:\n",
        "        return {\"NP\": 0, \"MPS_px\": 0.0}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab285f60",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 2.5. UDF - Matriz de Transición de Cobertura\n",
        "\n",
        "<small>Calcula una matriz de transición entre dos teselas de ráster (tiempo 1 y tiempo 2). Codifica cada transición de píxel en un número único (`origen * 100 + destino`) para facilitar la agregación posterior.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad9723a",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# UDF: transiciones entre dos rásters\n",
        "def get_transition_udf(raster_t1, raster_t2):\n",
        "    try:\n",
        "        d1 = raster_t1.as_numpy()[0].astype(np.int32)\n",
        "        d2 = raster_t2.as_numpy()[0].astype(np.int32)\n",
        "        trans = (d1 * 100) + d2\n",
        "        unique, counts = np.unique(trans, return_counts=True)\n",
        "        return {int(k): int(v) for k, v in zip(unique, counts) if not np.isnan(k)}\n",
        "    except Exception:\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a409548",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 2.6. Registro de UDFs en Spark\n",
        "<small>Este paso hace que las funciones de Python definidas anteriormente estén disponibles en el entorno SQL de Spark. Se especifica el nombre de la función y el esquema de datos que devuelve, lo que permite a Spark optimizar su ejecución.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c43e0e9",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# --- Registro de UDFs en Spark ---\n",
        "spark.udf.register('get_composition', get_composition_udf, MapType(IntegerType(), LongType()))\n",
        "fragmentation_schema = StructType([\n",
        "    StructField(\"NP\", IntegerType(), True),\n",
        "    StructField(\"MPS_px\", DoubleType(), True)\n",
        "])\n",
        "spark.udf.register('get_fragmentation', get_fragmentation_udf, fragmentation_schema)\n",
        "spark.udf.register('get_transition', get_transition_udf, MapType(IntegerType(), LongType()))\n",
        "print('Spark y UDFs (get_composition, get_fragmentation, get_transition) registrados correctamente.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s3_md",
      "metadata": {},
      "source": [
        "### 3. Fase de Extracción (Carga de Raw Data)\n",
        "\n",
        "En esta fase, nos conectamos a la base de datos PostgreSQL y cargamos las tablas dimensionales que necesitaremos para el análisis:\n",
        "- **`mapbiomas_clases`**: La leyenda de clases de cobertura.\n",
        "- **`dpa_comuna_subdere`**: Las geometrías de las comunas de Chile.\n",
        "- **`mapbiomas_grid`**: La grilla de teselas que cubre el área de estudio.\n",
        "\n",
        "Los datos geométricos se leen como WKB (Well-Known Binary) y se convierten al tipo `Geometry` de Sedona usando `ST_GeomFromWKB`.\n",
        "\n",
        "<small>Los datos fueron extraídos de sus fuentes e integrados a PostgreSQL mediante la función `raster2psql` previamente y se generó una grilla vectorial a partir de los tiles 512x512 con la función PostGIS `ST_Envelope()`<small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s3_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de credenciales desde el archivo .env\n",
        "config = dotenv_values(env_path)\n",
        "jdbc_url = f\"jdbc:postgresql://{config.get('POSTGRES_HOST')}:{config.get('POSTGRES_PORT')}/{config.get('POSTGRES_DB')}\"\n",
        "jdbc_props = {'user': config.get('POSTGRES_USER'), 'password': config.get('POSTGRES_PASSWORD'), 'driver': 'org.postgresql.Driver'}\n",
        "\n",
        "# Carga de la tabla de clases de MapBiomas. Se cachea porque es pequeña y se usa en múltiples joins.\n",
        "df_clases = spark.read.jdbc(jdbc_url, 'uso_suelo.mapbiomas_clases', properties=jdbc_props).cache()\n",
        "\n",
        "# Carga de las geometrías de las comunas\n",
        "df_comunas = spark.read.jdbc(jdbc_url, 'dpa_limites.dpa_comuna_subdere', properties=jdbc_props)\n",
        "df_comunas.createOrReplaceTempView('raw_comunas')\n",
        "df_comunas = spark.sql('SELECT comuna_id, comuna, ST_GeomFromWKB(geometria) as geometria FROM raw_comunas')\n",
        "\n",
        "# Carga de la grilla de referencia\n",
        "df_grid = spark.read.jdbc(jdbc_url, 'uso_suelo.mapbiomas_grid', properties=jdbc_props)\n",
        "df_grid.createOrReplaceTempView('raw_grid')\n",
        "df_grid = spark.sql('SELECT grid_id, ST_GeomFromWKB(geometria) as geometria FROM raw_grid')\n",
        "\n",
        "print('Raw Data: Tablas dimensionales cargadas.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s4_md",
      "metadata": {},
      "source": [
        "### 4. Data Store (ODS) Curada\n",
        "El ODS es un DataFrame intermedio que integra todos los datos necesarios para el análisis. Los pasos son:\n",
        "1.  **Definir una Región de Interés (ROI):** Se especifica una lista de comunas para acotar el análisis.\n",
        "2.  **Join Espacial:** Se cruzan las geometrías de las comunas del ROI con la grilla completa para identificar solo las teselas (`grid_id`) que se intersectan con nuestra ROI.\n",
        "3.  **Ingesta de Rásters:** Se construye una consulta SQL dinámica para leer desde PostGIS únicamente las teselas ráster que pertenecen a la ROI. **La función `ST_AsGDALRaster(rast, 'GTiff')` es fundamental aquí**, ya que convierte el tipo `raster` de PostGIS a un `byte array` en formato GeoTIFF, que Sedona puede interpretar.\n",
        "4.  **Recargar:** La función `RS_FromGeoTiff` de Sedona convierte el `byte array` en un objeto ráster de Sedona, listo para ser procesado por las UDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s4_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Definir la Región de Interés (ROI) por nombre de comuna\n",
        "ROI = ['Ñiquén', 'Penco', 'Nacimiento', 'Laja', 'Tomé']\n",
        "test_mode = False # Poner en True para limitar a 1 tesela para pruebas rápidas\n",
        "\n",
        "# Filtrar las comunas de interés\n",
        "target = df_comunas.filter(F.col('comuna').isin(ROI)).cache()\n",
        "\n",
        "# 2. Join espacial para encontrar las teselas que se intersectan con el ROI\n",
        "roi_grids = df_grid.alias('g').join(target.alias('c'), F.expr('ST_Intersects(g.geometria, c.geometria)')).select('g.grid_id', 'c.comuna_id', 'c.comuna')\n",
        "\n",
        "if test_mode:\n",
        "    print(\"MODO TEST: Limitando a 1 tesela.\")\n",
        "    roi_grids = roi_grids.limit(1)\n",
        "\n",
        "roi_grids = roi_grids.cache()\n",
        "\n",
        "# 3. Construir filtro SQL para leer solo los rásters necesarios\n",
        "grid_ids = [str(r['grid_id']) for r in roi_grids.select('grid_id').distinct().collect()]\n",
        "grid_filter = f\"WHERE grid_id IN ({','.join(grid_ids)})\" if grid_ids else \"WHERE 1=0\"\n",
        "raster_query = f\"(SELECT grid_id, year, ST_AsGDALRaster(rast, 'GTiff') as gdal_rast FROM uso_suelo.mapbiomas_raster {grid_filter}) AS sub\"\n",
        "\n",
        "# Leer los datos ráster crudos desde la base de datos\n",
        "df_raster_raw = spark.read.jdbc(jdbc_url, raster_query, properties=jdbc_props)\n",
        "\n",
        "# 4. Unir y recargar los rásters para crear el ODS final\n",
        "df_raster_roi = df_raster_raw.join(roi_grids, on='grid_id', how='inner') \\\n",
        "    .select('grid_id', F.expr('RS_FromGeoTiff(gdal_rast)').alias('rast'), 'year', 'comuna').cache()\n",
        "print(f'ODS Creado: {df_raster_roi.count()} teselas cargadas para las comunas {ROI}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s5_md",
      "metadata": {},
      "source": [
        "### 5. Data Mart 1: Métrica de Composición de Cobertura\n",
        "Esta métrica calcula el área total (en hectáreas) para cada clase de cobertura de suelo (agregada a Nivel 2 de la leyenda de MapBiomas) por comuna y año.\n",
        "\n",
        "El flujo es el siguiente:\n",
        "1.  Se aplica la UDF `get_composition` a cada tesela para obtener un mapa de `{id_clase: conteo_pixeles}`.\n",
        "2.  Se usa `F.explode` para convertir el mapa en filas, creando una tabla larga (una fila por clase por tesela).\n",
        "3.  Se agrupan los resultados por `comuna`, `año` y `id_clase` para sumar los píxeles.\n",
        "4.  Se convierte el conteo de píxeles a hectáreas (asumiendo píxeles de 30x30m, 0.09 ha/píxel).\n",
        "5.  Se hace un join con `df_clases` y se extrae el nombre de la clase de Nivel 2 del campo JSON para la agregación final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s5_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Calculando Composición de Cobertura por Nivel 2...')\n",
        "# 1. Aplicar UDF y explotar el mapa resultante\n",
        "df_composition_base = df_raster_roi.select('year', 'comuna', F.explode(F.expr('get_composition(rast)')).alias('id_clase', 'px')) \\\n",
        "    .groupBy('year', 'comuna', 'id_clase').agg(F.sum('px').alias('total_px')) \\\n",
        "    .withColumn('area_ha', F.col('total_px') * 0.09) \\\n",
        "    .join(df_clases, on='id_clase', how='left')\n",
        "\n",
        "# 2. Agregar a Nivel 2 de la leyenda\n",
        "df_composition_n2 = df_composition_base.filter(F.get_json_object('clase', '$.nivel_2').isNotNull()) \\\n",
        "    .groupBy('comuna', 'year', F.get_json_object('clase', '$.nivel_2').alias('clase_n2')) \\\n",
        "    .agg(F.sum('total_px').alias('total_px'), F.sum('area_ha').alias('area_ha')) \\\n",
        "    .orderBy('comuna', 'year', 'clase_n2', F.desc('area_ha'))\n",
        "\n",
        "print('Composición de Cobertura Nivel 2 calculada:')\n",
        "display.display(df_composition_n2.toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s6_md",
      "metadata": {},
      "source": [
        "### 6. Data Mart 2: Métrica de Fragmentación de Bosque Nativo\n",
        "Esta sección calcula la fragmentación específicamente para las clases que componen el **Bosque Nativo (Nivel 3)**: Bosque Primario, Bosque Secundario y Bosque Achaparrado.\n",
        "\n",
        "Las métricas calculadas son:\n",
        "-   **Número de Parches (NP):** El número total de fragmentos de una clase.\n",
        "-   **Tamaño Medio del Parche (MPS):** El área promedio de esos fragmentos.\n",
        "\n",
        "El proceso es más complejo que el anterior porque se calcula una métrica para cada tipo de bosque por separado y luego se unifican los resultados:\n",
        "1.  Se define la lista de IDs de clase para el bosque nativo.\n",
        "2.  Se aplica la UDF `get_fragmentation` de forma dinámica para cada ID, generando columnas separadas para los resultados de cada tipo de bosque (ej. `frag_59`, `frag_60`).\n",
        "3.  Se extraen las métricas (NP, MPS) de las columnas de tipo `Struct`.\n",
        "4.  Se agrupan los resultados por comuna y año.\n",
        "5.  Finalmente, se utiliza la función `stack` de Spark SQL para \"des-pivotear\" la tabla, convirtiendo las múltiples columnas de métricas en filas, lo que resulta en una tabla final limpia y normalizada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e0897d4",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 6.1. Fragmentación: Definición de Clases y Filtrado Temporal\n",
        "<small>Se definen las clases de bosque nativo que serán el foco del análisis y se filtra el DataFrame para el rango de años de interés (2000-2024).</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db07564",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "print('Calculando Fragmentación para las clases de Bosque Nativo (Nivel 3)...')\n",
        "\n",
        "# 1. Definir clases de interés (Nivel 3: Bosques Nativos)\n",
        "nivel3_bosques_ids = [59, 60, 67] # 59: Primario, 60: Secundario, 67: Achaparrado\n",
        "clase_names = {\n",
        "    59: \"Bosque Primario\",\n",
        "    60: \"Bosque Secundario\",\n",
        "    67: \"Bosque Achaparrado\"\n",
        "}\n",
        "df_frag_cols = df_raster_roi.filter((F.col(\"year\") >= 2000) & (F.col(\"year\") <= 2024))\n",
        "display.display(df_frag_cols.toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53415714",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 6.2. Fragmentación: Aplicación Dinámica de UDF\n",
        "<small>Para cada clase de bosque, se invoca la UDF de fragmentación. Esto crea una tabla ancha donde cada columna (`frag_59`, `frag_60`, etc.) contiene un `Struct` con las métricas NP y MPS para una clase específica.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd184ec",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# 2. Crear una expresión de UDF para cada clase de bosque y aplicarlas en una sola pasada\n",
        "frag_expressions = []\n",
        "for target_id in nivel3_bosques_ids:\n",
        "    target_classes_json = json.dumps([target_id])\n",
        "    frag_expressions.append(F.expr(f\"get_fragmentation(rast, '{target_classes_json}')\").alias(f\"frag_{target_id}\"))\n",
        "\n",
        "# Aplicar todas las UDFs en una sola pasada, creando una columna por métrica\n",
        "df_frag_multi_col = df_frag_cols.select(\"comuna\", \"year\", *frag_expressions)\n",
        "display.display(df_frag_multi_col.toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a791afd1",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 6.3. Fragmentación: Extracción de Métricas\n",
        "<small>Una vez calculadas, las métricas anidadas dentro de los `Structs` se extraen a columnas de primer nivel para facilitar la posterior agregación (ej. `frag_59.NP` se convierte en una columna `NP_59`).</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7367d81b",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# 3. Extraer las métricas del struct. Ej: df.select('frag_59.NP')\n",
        "select_expr = [\"comuna\", \"year\"]\n",
        "for target_id in nivel3_bosques_ids:\n",
        "    select_expr.append(F.col(f\"frag_{target_id}.NP\").alias(f\"NP_{target_id}\"))\n",
        "    select_expr.append(F.col(f\"frag_{target_id}.MPS_px\").alias(f\"MPS_px_{target_id}\"))\n",
        "df_metrics_expanded = df_frag_multi_col.select(*select_expr)\n",
        "display.display(df_metrics_expanded.toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810bc70a",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 6.5. Fragmentación: Agregación por Comuna\n",
        "\n",
        "<small>Con las métricas en columnas separadas, se realiza una agregación a nivel de comuna y año. El número de parches (NP) se suma, mientras que para el tamaño medio del parche (MPS) se calcula un promedio.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a41f97",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# 4. Agregar los resultados (sumar NP, promediar MPS) por comuna y año\n",
        "agg_expressions = []\n",
        "for target_id in nivel3_bosques_ids:\n",
        "    agg_expressions.append(F.sum(f\"NP_{target_id}\").alias(f\"total_parches_{target_id}\"))\n",
        "    agg_expressions.append(F.round(F.avg(f\"MPS_px_{target_id}\"), 2).alias(f\"tamano_medio_parche_px_{target_id}\"))\n",
        "    agg_expressions.append(F.sum(f\"MPS_px_{target_id}\").alias(f\"Total_area_px_{target_id}\"))\n",
        "\n",
        "df_aggregated_metrics = df_metrics_expanded.groupBy(\"comuna\", \"year\").agg(*agg_expressions)\n",
        "print('Métricas de Fragmentación calculadas y agregadas:')\n",
        "display.display(df_aggregated_metrics.toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b422b92",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 6.6. Fragmentación: Normalización y Formato Final\n",
        "<small>El paso final consiste en usar la expresión `stack` de Spark SQL para des-pivotear la tabla (convertir las columnas de métricas en filas). Esto normaliza la estructura de datos, produciendo una tabla final limpia, con una fila por tipo de bosque, comuna y año.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6566d319",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# 5. Usar 'stack' para des-pivotear y finalizar la tabla\n",
        "stack_expr_parts = []\n",
        "for target_id in nivel3_bosques_ids:\n",
        "    stack_expr_parts.append(repr(clase_names[target_id]))\n",
        "    stack_expr_parts.append(f\"total_parches_{target_id}\")\n",
        "    stack_expr_parts.append(f\"tamano_medio_parche_px_{target_id}\")\n",
        "    stack_expr_parts.append(f\"Total_area_px_{target_id}\")\n",
        "\n",
        "stack_expression = f'stack({len(nivel3_bosques_ids)}, {\", \".join(stack_expr_parts)}) as (clase, total_parches, tamano_medio_parche_px, Total_area_px_raw)'\n",
        "df_final_frag = df_aggregated_metrics.selectExpr(\"comuna\", \"year\", stack_expression)\n",
        "df_final_frag = df_final_frag     .withColumn(\"area_total_ha\", F.round(F.col(\"Total_area_px_raw\") * 0.09, 2))     .select(\"comuna\", \"year\", \"clase\", \"total_parches\", \"tamano_medio_parche_px\", \"area_total_ha\")     .orderBy(\"comuna\", \"year\", \"clase\")\n",
        "display.display(df_final_frag.toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s7_md",
      "metadata": {},
      "source": [
        "### 7. Data Mart 3: Métrica de Transición de Cobertura\n",
        "Esta métrica construye una matriz de cambio que muestra cómo ha cambiado la cobertura del suelo entre dos años (en este caso, 2000 y 2024). \n",
        "\n",
        "Pasos del proceso:\n",
        "1.  Se filtran los datos del ODS para obtener dos DataFrames, uno para el año de inicio (`t1`) y otro para el año final (`t2`).\n",
        "2.  Se realiza un `join` entre `t1` y `t2` usando `grid_id` y `comuna` como clave. Esto empareja cada tesela con su versión correspondiente en el otro año.\n",
        "3.  Se aplica la UDF `get_transition` a cada par de teselas. La UDF codifica la transición de cada píxel en un número único (`origen * 100 + destino`).\n",
        "4.  Se explota el mapa de transiciones y se agrupa para obtener el conteo total de píxeles para cada tipo de transición (`origen -> destino`).\n",
        "5.  Se decodifican los IDs de clase de origen y destino.\n",
        "6.  Se realiza un doble join con la tabla de clases para obtener los nombres de las clases de origen y destino, y se agregan los resultados a Nivel 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s7_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Calculando Transiciones de Cobertura (2000 -> 2024) por Nivel 2...')\n",
        "# 1. Filtrar los años de inicio y fin\n",
        "t1 = df_raster_roi.filter(F.col('year') == 2000).alias('t1')\n",
        "t2 = df_raster_roi.filter(F.col('year') == 2024).alias('t2')\n",
        "\n",
        "# 2. Unir las teselas por su ID, aplicar la UDF de transición, explotar y agregar los resultados\n",
        "df_trans_base = t1.join(t2, on=['grid_id', 'comuna']) \\\n",
        "    .select(\"comuna\", F.expr(\"get_transition(t1.rast, t2.rast)\").alias(\"transicion_data\")) \\\n",
        "    .select(\"comuna\", F.explode(\"transicion_data\").alias(\"codigo_transicion\", \"total_px_transicion\")) \\\n",
        "    .groupBy(\"comuna\", \"codigo_transicion\").agg(F.sum(\"total_px_transicion\").alias(\"total_px\")) \\\n",
        "    .withColumn(\"from_id_clase\", (F.col(\"codigo_transicion\") / 100).cast('integer')) \\\n",
        "    .withColumn(\"to_id_clase\", (F.col(\"codigo_transicion\") % 100).cast('integer')) \\\n",
        "    .withColumn(\"area_ha\", F.round(F.col(\"total_px\") * 0.09, 2)) \\\n",
        "    .join(df_clases.alias(\"clase_origen\"), F.col(\"from_id_clase\") == F.col(\"clase_origen.id_clase\"), \"left\") \\\n",
        "    .join(df_clases.alias(\"clase_destino\"), F.col(\"to_id_clase\") == F.col(\"clase_destino.id_clase\"), \"left\")\n",
        "\n",
        "# 3. Agregar a Nivel 2 y mostrar\n",
        "df_trans_n2 = df_trans_base.filter(\n",
        "        F.get_json_object(F.col(\"clase_origen.clase\"), '$.nivel_2').isNotNull() & \\\n",
        "        F.get_json_object(F.col(\"clase_destino.clase\"), '$.nivel_2').isNotNull()\n",
        "    ) \\\n",
        "    .groupBy(\n",
        "        \"comuna\", \n",
        "        F.get_json_object(F.col(\"clase_origen.clase\"), '$.nivel_2').alias(\"clase_origen\"), \n",
        "        F.get_json_object(F.col(\"clase_destino.clase\"), '$.nivel_2').alias(\"clase_transicion\")\n",
        "    ) \\\n",
        "    .agg(F.sum('total_px').alias('total_px_transicion'), F.sum('area_ha').alias('area_ha_transicion')) \\\n",
        "    .orderBy(\"comuna\", F.desc(\"area_ha_transicion\"))\n",
        "\n",
        "display.display(df_trans_n2.toPandas())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
